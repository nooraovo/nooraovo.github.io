---
title:          "Initialization Matters: On the Benign Overfitting of Two-Layer ReLU CNN with Fully Trainable Layers"
date:           2024-10-24 00:01:00 +0800
selected:       true
# pub:            "International Conference on Machine Learning (ICML)"
# pub_pre:        "Submitted to "
pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
  We study benign overfitting in single-head attention, the core of Transformers. We show that under certain conditions, the model can fit noisy training data and still generalize well, even after just two steps of gradient descent. Our results highlight the key role of the signal-to-noise ratio in enabling this behavior.
cover:          /assets/images/covers/singlehead.JPG
authors:
  - Shuning Shang
  - Xuran Meng
  - Yuan Cao
  - Difan Zou
links:
  Paper: https://arxiv.org/abs/2410.19139
---
