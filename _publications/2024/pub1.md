---
title:          "Initialization Matters: On the Benign Overfitting of Two-Layer ReLU CNN with Fully Trainable Layers"
date:           2024-10-24 00:01:00 +0800
selected:       false
# pub:            "International Conference on Machine Learning (ICML)"
# pub_pre:        "Submitted to "
pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
  We study benign overfitting in two-layer ReLU CNNs with all layers trained, going beyond prior work that fixes the output layer. We show that the output layer's initialization scale crucially affects training dynamics and generalization. Our analysis provides sharp conditions under which benign overfitting occurs, supported by matching bounds and experiments.
cover:          /assets/images/covers/initializationmatters.JPG
authors:
  - Shuning Shang
  - Xuran Meng
  - Yuan Cao
  - Difan Zou
links:
  Paper: https://arxiv.org/abs/2410.19139
---
